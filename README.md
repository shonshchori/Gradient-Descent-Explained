# Gradient Descent for Simple Linear Regression
This repository contains Python code for implementing gradient descent, stochastic gradient descent (SGD), and mini-batch gradient descent (MBGD) algorithms for simple linear regression. The goal of the algorithms is to find the optimal values of the intercept and slope that minimize the sum of squared errors between the predicted and actual values of the target variable.

## Features
* Gradient descent: iteratively updates the parameters by taking small steps in the direction of steepest descent of the loss function.
* SGD: updates the parameters using the gradient of a single randomly selected sample at each iteration.
* MBGD: updates the parameters using the gradient of a small batch of randomly selected samples at each iteration.  
---
The code is written in Python. It includes examples of how to use the algorithms to fit a linear regression model to a dataset and plot the results. The included notebook provides clear explanations and visualizations of the algorithms and the data.
